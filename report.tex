\subsection*{Disclaimer}
In this report, I adopt a writing convention:
\begin{itemize}
    \item Filename are in \textit{italic}
    \item Any value, variable, string, object ... is in \textbf{boldface} 
\end{itemize}
Please note that the output files are quite big, I will provide the first ten results of each language in the report, the complete list of results will be available on the \href{https://github.com/PaxOwl/big-data-project}{github page} of this project in \textit{project/data\_out/}.
\section{Introduction}

\newpage
\section{Task 1}
\label{sec:task1}
In Graph Theory, a graph is a structure made of vertices connected by edges. These graph can be directed (asymmetrically) and undirected (symmetrically). The PageRank method uses the graph theory to rank pages of the web by analyzing the hyperlinks between pages. In this part we will see how this algorithm works from a matrix approach using the following graph.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.5\linewidth]{network.pdf}
    \caption{Graph used to experiment with the algorithm}\label{fig:network}
\end{figure}

The first mathematical object needed is the adjacency matrix. It is a matrix encoding the structure of the graph. This matrix is defined by:
\begin{equation}
    A_{ij} = \left\{\begin{array}{ll}
        1 & $if $j$ point to $i\\
        0 & \text{otherwise}
    \end{array} \right.
\end{equation}
The adjacency matrix of the considered graph is:
\[
    \begin{pmatrix}
        0 & 0 & 1 & 0 & 0 & 0\\
        1 & 0 & 1 & 0 & 0 & 0\\
        1 & 0 & 0 & 0 & 0 & 0\\
        0 & 0 & 0 & 0 & 1 & 1\\
        0 & 0 & 1 & 1 & 0 & 0\\
        0 & 0 & 0 & 1 & 1 & 0\\
    \end{pmatrix}
\]
Since our graph is directed, this matrix is not symmetrical.

Once the adjacency matrix is build, we need the stochastic matrix:
\begin{equation}
    S_{ij} = \left\{ \begin{array}{ll}
        A_{ij} / k_j^{\text{out}} & \text{if } k_j^{\text{out}} \neq 0\\
        1 / N & \text{otherwise} 
    \end{array} \right. \text{ with } k_j^{\text{out}} = \displaystyle\sum_{i=1}^{N} A_{ij}
\end{equation}
The out-degree $k_j^{\text{out}}$ corresponds to the number of vertices to leave the vertex $j$. An element $S_{ij}$ of the stochastic matrix corresponds to the probability of jumping from node $i$ to node $j$. The second part in the definition od $S$ is there is case we end up on a node without any escape ($k_{\text{out}} = 0$). In that case we have a probability $1/N$ of jumping into any node of the network.

The out-degree $k^{\text{out}}$ and the stochastic matrix $S$ of the considered graph are:
\[  
    k^{\text{out}} = \begin{pmatrix}
        2\\
        0\\
        3\\
        2\\
        2\\
        1\\
    \end{pmatrix}
    \qquad \qquad
    S = \begin{pmatrix}
        0 & 1/6 & 1/3 & 0 & 0 & 0\\
        1/2 & 1/6 & 1/3 & 0 & 0 & 0\\
        1/2 & 1/6 & 0 & 0 & 0 & 0\\
        0 & 1/6 & 0 & 0 & 1/2 & 1\\
        0 & 1/6 & 1/3 & 1/2 & 0 & 0\\
        1 & 1/6 & 0 & 1/2 & 1/2 & 0\\
    \end{pmatrix}
\]
Now that we have the probability to jump into a node $j$ from any node $i$, we can define the initial probability distribution $\mathbf{p}^{(0)}$ which depends on the starting node $j_0$.
\begin{equation}
    p_i^{(0)} = \delta_{ij_0}
\end{equation}

Let us introduce the Perron-Frobenius operator $G$:
\begin{equation}
    G_{ij} = \alpha S_{ij} + (1 - \alpha) v_i
\end{equation}
Where $\alpha$ is the damping factor and \textbf{v} a preferential vector. By convention, we take $\alpha = \num{0.85}$, but it could be anything in the interval $[0.5, 1[$. The values of the preferential vector \textbf{v} are all the same: $v_i = 1/N$.

By applying $G$ onto \textbf{p} an infinite number of time (i.e. $G^{\infty}\mathbf{p}^{(0)}$) we find the steady state probability distribution \textbf{P}.

The computation of the Google matrix in the example network gives:
\begin{equation}
    G = \begin{pmatrix}
        0.025 & 1/6 & 0.308 & 0.025 & 0.025 & 0.025\\
        0.450 & 1/6 & 0.308 & 0.025 & 0.025 & 0.025\\
        0.450 & 1/6 & 0.025 & 0.025 & 0.025 & 0.025\\
        0.025 & 1/6 & 0.025 & 0.025 & 0.450 & 0.875\\
        0.025 & 1/6 & 0.208 & 0.450 & 0.025 & 0.025\\
        0.025 & 1/6 & 0.025 & 0.451 & 0.451 & 0.025\\
    \end{pmatrix}
\end{equation}

Once we have \textbf{P}, we have to sort it in decreasing order and we have the ranking of our network. We now have all that is needed to solve the eigenproblem and rank the network.


After solving the eigenproblem, the results are as follows:

\begin{table}[htbp]
    \begin{minipage}{.25\linewidth}
        \centering
        \begin{tabular}{ll}
            \toprule
            Node & Rank\\
            \midrule
            4 & 1\\
            6 & 2\\
            5 & 3\\
            2 & 4\\
            3 & 5\\
            1 & 6\\
            \bottomrule
        \end{tabular}
        \caption{Ranking for \autoref{fig:network}}\label{tab:node}
    \end{minipage}
    \hfill
    \begin{minipage}{.65\linewidth}
        \centering
        \begin{tabular}{ccccccc}
            \toprule
            & \multicolumn{6}{c}{Probability}\\
            \cmidrule{2-7}
            $\alpha$ & Node 1 & Node 2 & Node 3 & Node 4 & Node 5 & Node 6\\
            \midrule
            0.50 & 0.11622 & 0.14530 & 0.12452 & 0.23893 & 0.17590 & 0.19910\\
            0.55 & 0.10942 & 0.13953 & 0.11791 & 0.24966 & 0.17806 & 0.20539\\
            0.60 & 0.10207 & 0.13270 & 0.11058 & 0.26159 & 0.18050 & 0.21253\\
            0.65 & 0.09408 & 0.12468 & 0.10246 & 0.27480 & 0.18330 & 0.22065\\
            0.70 & 0.08520 & 0.11503 & 0.09326 & 0.28979 & 0.18658 & 0.23012\\
            0.75 & 0.07541 & 0.10372 & 0.08295 & 0.30665 & 0.19031 & 0.24093\\
            0.80 & 0.06433 & 0.09008 & 0.07110 & 0.32611 & 0.19472 & 0.25364\\
            0.85 & 0.05183 & 0.07390 & 0.05756 & 0.34846 & 0.19981 & 0.26840\\
            0.90 & 0.03732 & 0.05415 & 0.04163 & 0.37486 & 0.20592 & 0.28608\\
            0.95 & 0.02035 & 0.03005 & 0.02281 & 0.40623 & 0.21324 & 0.30728\\
            0.99 & 0.00447 & 0.00671 & 0.00503 & 0.43599 & 0.22022 & 0.32754\\
            \bottomrule
        \end{tabular}
        \caption{Results for different values of $\alpha$}\label{tab:alpha}
    \end{minipage}
\end{table}

Looking at \autoref{tab:alpha}, the action of the damping factor $\alpha$ on the calculation of the steady state probability is clear. As $\alpha$ increases, the values for the first 3 nodes decreases while the values of the last 3 nodes increases. By taking into consideration the damping factor in the computations, we prevent the program to stuck itself on a the last 3 nodes, therefore producing more accurate results.
\section{Task 2}
The approach of the adjacency matrix we saw in \autoref{sec:task1} is satisfying for small networks. However, with a network or several thousand nodes, solving for the eigenvalues of a matrix is not a conceivable method, the computation time and the load on the memory would be to high. A new method is needed, the power iteration method.

\subsection{About the program}
    In order to build a PageRank algorithm using the power iteration method, I choosed to use object oriented programming. My program is composed of multiple files:
    \begin{itemize}
        \item \textit{main.py}: ``Controller'' of the program
        \item \textit{parameters1.py}: Parameters to use for the task 1
        \item \textit{parameters2.py}: Parameters to use for the task 2
        \item \textit{function\_task1.py}: Contains the \textbf{class} \textbf{NetworkTask1} that computes the rank using the adjacency matrix method
        \item \textit{function\_task2.py}: Contains the \textbf{class} \textbf{NetworkTask2} that computes the rank using the power iteration method
    \end{itemize}

\subsection{\textit{main.py}}
This file is made of 2 parts, depending on the value of \textbf{run\_all\_files} (bool) of \textit{main.py}.

If \textbf{False}, it will run the task 1 and the task 2 on the small network (\textit{network\_data.txt}) and print the rank of each node for both tasks.

If \textbf{True}, it will run only the task 2 on a list of files (\textbf{files} in \textit{parameters2.py}), printing several informations about the computation process and save logs containing time-stamp and the number of iterations needed for the convergence of P (\textit{logs.log} is a file containing all the logs, the starting and ending time of the program, it also saves a log file for each computed network \textit{``filename''.log}).

\subsection{\textit{parameters2.py}}
This file contains the constants to use for the task 2:
\begin{itemize}
    \item The damping factor \textbf{alpha}
    \item The criterion of convergence precision \textbf{epsilon}
    \item The list of files to use \textbf{files}
\end{itemize}

\subsection{functions\_task2.py}
This is the core of the program, where all the computations are made.

I start by defining a \textbf{class} \textbf{NetworkTask2} which is kind of a ``storage'' for all the values of the network.

Each function defined in the \textbf{class} measures and saves the execution time in a string \textbf{self.log}

There are some functions when I am forced to use lists instead of arrays (mostly because we cannot know the size of the array beforehand), I made sure to always return arrays, which are less expensive in memory.

I will not comment on the time-stamp lines in the pseudo-code since they are not relevant for the algorithm, they are here to keep track of the state of the program.
\subsubsection{\textbf{\_\_init\_\_}}
This function is called when initiating the \textbf{class}, it is merely here to show what names are used for the different variables since I don't want to initiate them at \textbf{class} call.

\subsubsection{\textbf{compute}}
This function calls the other functions to compute each \textbf{attribute} of the \textbf{class}.

\begin{table}[htbp]
    \centering
    \begin{tabular}{rlrl}
        \toprule
        \multicolumn{2}{c}{Input} & \multicolumn{2}{c}{Output}\\
        \midrule
        \tabitem & \textbf{filename (str)}: the name & \tabitem & \textbf{None}\\
        & of the file to use & &\\
        \bottomrule
    \end{tabular}
    \caption{\textbf{load\_data} function variable}\label{tab:compute}
\end{table}

\subsubsection{\textbf{load\_data}}
This function loads the data stored in \textbf{filename} and returns it in a array along with the number of nodes.

\begin{table}[htbp]
    \centering
    \begin{tabular}{rlrl}
        \toprule
        \multicolumn{2}{c}{Input} & \multicolumn{2}{c}{Output}\\
        \midrule
        \tabitem & \textbf{filename (str)}: the name & \tabitem & \textbf{data (np.ndarray)}: the links\\
        & of the file to use & & between the nodes\\
        & & \tabitem & \textbf{n\_node (int)}: the number\\
        & & & of nodes in the network\\
        \bottomrule
    \end{tabular}
    \caption{\textbf{load\_data} function variables}\label{tab:load-data}
\end{table}

\subsubsection{\textbf{build\_degree}}
This function builds \textbf{k\_out}

\begin{table}[htbp]
    \centering
    \begin{tabular}{rlrl}
        \toprule
        \multicolumn{2}{c}{Input} & \multicolumn{2}{c}{Output}\\
        \midrule
        \multicolumn{2}{c}{\textbf{None}} & \tabitem & \textbf{k\_out (dict)}: the number of\\
        & & & ways to leave each node\\
        \bottomrule
    \end{tabular}
    \caption{\textbf{build\_degree} function variables}\label{tab:build-degree}
\end{table}

It works by using the function \textbf{Counter()} of the package \textbf{collections} which counts the occurences for each value in the column 1 and stores them in a sort of \textbf{dictionary} (it is a specific type from the package \textbf{collections}, but easily converted to a \textbf{dictionary}) according to the following pattern:

\mintinline{python}{{node1: occurence, node2: occurence, ..., nodeN: occurence}}

\subsubsection{\textbf{build\_dangling}}
This function store the number associated to each dangling node (node without any way to leave).

\begin{table}[htbp]
    \centering
    \begin{tabular}{rlrl}
        \toprule
        \multicolumn{2}{c}{Input} & \multicolumn{2}{c}{Output}\\
        \midrule
        \multicolumn{2}{c}{\textbf{None}} & \tabitem & \textbf{dangling (np.ndarray)}: the number of\\
        & & & ways to leave each node\\
        \bottomrule
    \end{tabular}
    \caption{\textbf{build\_degree} function variables}\label{tab:build-dangling}
\end{table}

It makes use of the dictionary \textbf{k\_out} and the number of nodes to build \textbf{k\_out}. The function iterates on the number of nodes and checks if there is an entry for the current node. If there is none, it appends the current node to the list dangling.

\subsubsection{\textbf{build\_p}}
This function computes the steady state probability \textbf{p} of each node.

\begin{table}[htbp]
    \centering
    \begin{tabular}{rlrl}
        \toprule
        \multicolumn{2}{c}{Input} & \multicolumn{2}{c}{Output}\\
        \midrule
        \tabitem & \textbf{filename}:  the name & \tabitem & \textbf{p (np.ndarray)}: the steady\\
        & of the file to use & & state probability of each node\\
        \bottomrule
    \end{tabular}
    \caption{\textbf{build\_p} function variables}\label{tab:build-p}
\end{table}

This function is the longest of the program, for it does several long tasks. It starts by initiating \textbf{gp} as an array the size of the network using the following definition : $Gp(i)_N = \dfrac{1}{N}$ with $N$ the number of nodes in the network. It then enters an infinite do while loop where it computes \textbf{p} until $||\mathbf{gp} - \mathbf{p}|| > \text{\textbf{epsilon}}$ or if the iterations limit (set at 1000) is reached.

\underline{Do While details}:
\begin{enumerate}
    \item Store a copy of \textbf{gp} in the variable \textbf{p}
    \item Parse the list of links (\textbf{self.data}) and for each link A $\rightarrow$ B increase $\mathbf{gp}(\text{B})$ by $\dfrac{\alpha * \mathbf{p}(\text{A})}{\mathbf{k\_out}(\text{A})}$
    \item Parse the list of dangling nodes (\textbf{self.dangling}) and for each dangling node C increases $\mathbf{gp}(\text{i})$ by $\dfrac{\alpha * \mathbf{p}(\text{C})}{N}$
    \item At each iteration, increase $\mathbf{gp}(\text{i})$ by $\dfrac{1 - \alpha}{N}$
    \item Divide $\mathbf{gp}$ by its norm.
    \item Perform the checks, if the criterion of convergence is met or if the number of iterations is higher than 1000, the loop stops. Else it starts over with the newly computed \textbf{gp}.
\end{enumerate}

After the loop, it returns the array \textbf{m}.

\subsubsection{\textbf{build\_index}}
Given the steady state probability \textbf{p} for each node, this function computes the rank of each node and sorts them.

\begin{table}[htbp]
    \centering
    \begin{tabular}{rlrl}
        \toprule
        \multicolumn{2}{c}{Input} & \multicolumn{2}{c}{Output}\\
        \midrule
        \tabitem & \textbf{filename}:  the name & \tabitem & \textbf{k (np.ndarray)}: each node\\
        & of the file to use & & and their rank\\
        \bottomrule
    \end{tabular}
    \caption{\textbf{build\_index} function variables}\label{tab:build-index}
\end{table}

This function initiates \textbf{k} as an empty list and appends a tuple (\textbf{node}, \textbf{p}) for each node. It then converts this list into an array and sorts it by probability (higher \textbf{p} first). Then it iterates on the number of nodes and assign a rank to each node (replace \textbf{p}(i) with \textbf{k}(i)).

\underline{About the means used in Python}

\hfill
\begin{minipage}{.95\linewidth}
    In order for numpy to sort an array by a column, we must make use of a structured array. However, I could not simply create a structured array, so I built a list a tuples as said above. I then convert it to an array using the argument \textbf{dtype} to assign the columns to their name (\textbf{node} and \textbf{rank}). With that, I can tell numpy to sort the array by order of \textbf{rank}. Since it sorts from lower to higher, I flip the list to have the wanted order. After that, all that is left is to assign the ranks in order and save the array.

\end{minipage}
\section{Performances comparison}
Running both task on the 6-nodes network brings up the following results:
\begin{table}[htbp]
    \centering
    \begin{tabular}{lcc}
        \toprule
        & Task 1 & Task 2\\
        \midrule
        Computation time & \SI{1.383}{\milli\second} & \SI{3.248}{\milli\second}\\
        \bottomrule
    \end{tabular}
    \caption{Ranks and computing time for both tasks}\label{tab:time-comparison}
\end{table}

 It appears that the task 1 is faster than the task 2. It is probably because the network is to small, but the smallest sample of data we had (\textit{thwiki.txt}) was already too big for the computation (it needs something like \SI{45}{\giga B} of memory). Regarding the probabilities and ranks, here are the results:

\begin{table}[htbp]
    \centering
    \begin{tabular}{ccccc}
        \toprule
        & \multicolumn{2}{c}{Probability} & \multicolumn{2}{c}{Rank}\\
        \cmidrule(l{2pt}r{2pt}){2-3} \cmidrule(l{2pt}r{2pt}){4-5}
        Node & Task 1 & Task 2 & Task 1 & Task 2\\
        \midrule
        1 & 0.05183647 & 0.05170476 & 6 & 6\\
        2 & 0.07390771 & 0.07367929 & 4 & 4\\
        3 & 0.05756534 & 0.05741243 & 5 & 5\\
        4 & 0.34846758 & 0.34870366 & 1 & 1\\
        5 & 0.19981617 & 0.19990381 & 3 & 3\\
        6 & 0.26840673 & 0.26859606 & 2 & 2\\
        \bottomrule
    \end{tabular}
    \caption{Probabilities and ranks for both tasks}\label{tab:prob-rank-comparison}
\end{table}

The ranks are totally identical while the probabilities are only identical up to a certain degree of precision. It is set by $\epsilon = \num{e-4}$ which is the limit .
\section{Task 3}
Now that we have a full-functional algorithm to compute the PageRank, it is time to use it on real data. We have a list of files containing the hyperlinks between the Wikipedia pages of 24 languages. We are interested in the first 10 pages of each language, a comparison of the rank of the same page in different language and the evolution of the CPU time needed in function of the number of links.

\subsection{Top 10 articles}
\begin{table}[htbp]
    \begin{minipage}{.30\linewidth}
        \begin{tabular}{ll}
            \toprule
            Article & Rank\\
            \midrule
            الولايات المتحدة (310) & 1\\
            مصر (61633) & 2\\
            لغة إنجليزية (129120) & 3\\
            فرنسا (250) & 4\\
            إسبانيا (637) & 5\\
            أوروبا (779) & 6\\
            إيطاليا (571) & 7\\
            المملكة المتحدة (1493) & 8\\
            روسيا (5996) & 9\\
            ألمانيا (35) & 10\\
            \bottomrule
        \end{tabular}
        \caption{Top 10 articles for the Arabic edition}
    \end{minipage}
\end{table}

\section{Conclusion}
